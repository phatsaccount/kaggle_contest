{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237ed4ef",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007695,
     "end_time": "2025-12-05T15:47:02.311394",
     "exception": false,
     "start_time": "2025-12-05T15:47:02.303699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**References**\n",
    "- [MABe Nearest Neighbors: The Original â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸](https://www.kaggle.com/code/ambrosm/mabe-nearest-neighbors-the-original)\n",
    "- [MABe EDA which makes sense â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸](https://www.kaggle.com/code/ambrosm/mabe-eda-which-makes-sense)\n",
    "- [MABe Validated baseline without machine learning](https://www.kaggle.com/code/ambrosm/mabe-validated-baseline-without-machine-learning)\n",
    "- [Squeeze GBT](https://www.kaggle.com/code/cody11null/squeeze-gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8390b",
   "metadata": {
    "papermill": {
     "duration": 0.005922,
     "end_time": "2025-12-05T15:47:02.323501",
     "exception": false,
     "start_time": "2025-12-05T15:47:02.317579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6765bf19",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:02.337477Z",
     "iopub.status.busy": "2025-12-05T15:47:02.337250Z",
     "iopub.status.idle": "2025-12-05T15:47:12.046328Z",
     "shell.execute_reply": "2025-12-05T15:47:12.045611Z"
    },
    "papermill": {
     "duration": 9.717876,
     "end_time": "2025-12-05T15:47:12.047651",
     "exception": false,
     "start_time": "2025-12-05T15:47:02.329775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pip-koolbox/koolbox\r\n",
      "Processing /kaggle/input/pip-koolbox/koolbox/koolbox-0.1.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pip-koolbox/koolbox/scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from koolbox)\r\n",
      "Requirement already satisfied: optuna>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from koolbox) (4.5.0)\r\n",
      "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from koolbox) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from koolbox) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2.4.1)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (1.17.1)\r\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (6.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (25.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (2.0.41)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (4.67.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (6.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (3.6.0)\r\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=4.2.1->koolbox) (1.3.10)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=4.2.1->koolbox) (4.15.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->koolbox) (1.17.0)\r\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=4.2.1->koolbox) (3.2.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->koolbox) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna>=4.2.1->koolbox) (3.0.3)\r\n",
      "Installing collected packages: scikit-learn, koolbox\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed koolbox-0.1.3 scikit-learn-1.7.2\r\n"
     ]
    }
   ],
   "source": [
    "# Lá»‡nh nÃ y báº£o mÃ¡y tÃ­nh: \"Äá»«ng lÃªn máº¡ng táº£i, hÃ£y tÃ¬m file cÃ i Ä‘áº·t trong thÆ° má»¥c nÃ y\"\n",
    "!pip install koolbox --no-index --find-links=/kaggle/input/pip-koolbox/koolbox\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
    "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "    for row in lab_solution.to_dicts():\n",
    "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
    "\n",
    "    for video in lab_solution['video_id'].unique():\n",
    "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
    "        active_labels: set[str] = set(json.loads(active_labels))\n",
    "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
    "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
    "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
    "                continue\n",
    "\n",
    "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
    "            # Ignore truly redundant predictions.\n",
    "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
    "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
    "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
    "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
    "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
    "            prediction_frames[row['prediction_key']].update(new_frames)\n",
    "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
    "\n",
    "    tps = defaultdict(int)\n",
    "    fns = defaultdict(int)\n",
    "    fps = defaultdict(int)\n",
    "    for key, pred_frames in prediction_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        matched_label_frames = label_frames[key]\n",
    "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
    "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
    "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
    "\n",
    "    distinct_actions = set()\n",
    "    for key, frames in label_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        if key not in prediction_frames:\n",
    "            fns[action] += len(frames)\n",
    "\n",
    "    action_f1s = []\n",
    "    for action in distinct_actions:\n",
    "        if tps[action] + fns[action] + fps[action] == 0:\n",
    "            action_f1s.append(0)\n",
    "        else:\n",
    "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
    "    return sum(action_f1s) / len(action_f1s)\n",
    "\n",
    "\n",
    "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    Doctests:\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 0, 'stop_frame': 10}, # Wrong action\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    0.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
    "    ... ])\n",
    "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
    "    '0.500000000000'\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 345, 'agent_id': 1, 'target_id': 2, 'action': 'mount', 'start_frame': 15, 'stop_frame': 24, 'lab_id': 2, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 123, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 9},\n",
    "    ... ])\n",
    "    >>> \"%.12f\" % mouse_fbeta(solution, submission)\n",
    "    '0.250000000000'\n",
    "\n",
    "    >>> # Overlapping solution events, one prediction matching both.\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 10, 'stop_frame': 20, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 20},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 10, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 30, 'stop_frame': 40, 'lab_id': 1, 'behaviors_labeled': '[\"1,2,attack\"]'},\n",
    "    ... ])\n",
    "    >>> submission = pd.DataFrame([\n",
    "    ...     {'video_id': 1, 'agent_id': 1, 'target_id': 2, 'action': 'attack', 'start_frame': 0, 'stop_frame': 40},\n",
    "    ... ])\n",
    "    >>> mouse_fbeta(solution, submission)\n",
    "    0.6666666666666666\n",
    "    \"\"\"\n",
    "    if len(solution) == 0 or len(submission) == 0:\n",
    "        raise ValueError('Missing solution or submission data')\n",
    "\n",
    "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "    for col in expected_cols:\n",
    "        if col not in solution.columns:\n",
    "            raise ValueError(f'Solution is missing column {col}')\n",
    "        if col not in submission.columns:\n",
    "            raise ValueError(f'Submission is missing column {col}')\n",
    "\n",
    "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
    "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
    "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
    "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
    "    solution_videos = set(solution['video_id'].unique())\n",
    "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
    "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
    "\n",
    "    solution = solution.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key'),\n",
    "    )\n",
    "    submission = submission.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key'),\n",
    "    )\n",
    "\n",
    "    lab_scores = []\n",
    "    for lab in solution['lab_id'].unique():\n",
    "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
    "        lab_videos = set(lab_solution['video_id'].unique())\n",
    "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
    "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
    "\n",
    "    return sum(lab_scores) / len(lab_scores)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    F1 score for the MABe Challenge\n",
    "    \"\"\"\n",
    "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    return mouse_fbeta(solution, submission, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11600da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:12.062646Z",
     "iopub.status.busy": "2025-12-05T15:47:12.062319Z",
     "iopub.status.idle": "2025-12-05T15:47:14.217453Z",
     "shell.execute_reply": "2025-12-05T15:47:14.216808Z"
    },
    "papermill": {
     "duration": 2.164034,
     "end_time": "2025-12-05T15:47:14.218743",
     "exception": false,
     "start_time": "2025-12-05T15:47:12.054709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from koolbox import Trainer\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "from scipy import signal, stats\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f93d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.234114Z",
     "iopub.status.busy": "2025-12-05T15:47:14.233706Z",
     "iopub.status.idle": "2025-12-05T15:47:14.238364Z",
     "shell.execute_reply": "2025-12-05T15:47:14.237704Z"
    },
    "papermill": {
     "duration": 0.013369,
     "end_time": "2025-12-05T15:47:14.239589",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.226220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n",
    "    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    model_path = \"/kaggle/input/xgboost-threshold\"\n",
    "    model_name = \"xgboost\"\n",
    "    \n",
    "    # mode = \"validate\"\n",
    "    mode = \"submit\"\n",
    "    \n",
    "    n_splits = 3\n",
    "    cv = StratifiedGroupKFold(n_splits)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        verbosity=0, \n",
    "        random_state=42,\n",
    "        n_estimators=250, \n",
    "        learning_rate=0.08, \n",
    "        max_depth=6,\n",
    "        min_child_weight=5, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3179d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.255178Z",
     "iopub.status.busy": "2025-12-05T15:47:14.254881Z",
     "iopub.status.idle": "2025-12-05T15:47:14.325276Z",
     "shell.execute_reply": "2025-12-05T15:47:14.324296Z"
    },
    "papermill": {
     "duration": 0.079565,
     "end_time": "2025-12-05T15:47:14.326614",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.247049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying model path...\n",
      "\n",
      "Model path: /kaggle/input/xgboost-threshold/xgboost\n",
      "Exists: True\n",
      "\n",
      "âœ… Sections found: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "\n",
      "Section 1:\n",
      "  âœ… approach: 1 fold models\n",
      "  âœ… attack: 1 fold models\n",
      "  âœ… avoid: 1 fold models\n",
      "  ... and 4 more actions\n",
      "\n",
      "Section 2:\n",
      "  âœ… huddle: 1 fold models\n",
      "  âœ… reciprocalsniff: 1 fold models\n",
      "  âœ… sniffgenital: 1 fold models\n",
      "\n",
      "Section 3:\n",
      "  âœ… approach: 1 fold models\n",
      "  âœ… attack: 1 fold models\n",
      "  âœ… avoid: 1 fold models\n",
      "  ... and 4 more actions\n",
      "\n",
      "ðŸ“Š Total trainer files: 9\n",
      "âœ… Thresholds file loaded\n",
      "   Keys: ['single', 'pair']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFY MODEL FILES (RUN THIS FIRST IN SUBMIT MODE)\n",
    "# =============================================================================\n",
    "print(\"ðŸ” Verifying model path...\\n\")\n",
    "\n",
    "model_base = f\"{CFG.model_path}/{CFG.model_name}\"\n",
    "print(f\"Model path: {model_base}\")\n",
    "print(f\"Exists: {os.path.exists(model_base)}\\n\")\n",
    "\n",
    "if os.path.exists(model_base):\n",
    "    sections = sorted([d for d in os.listdir(model_base) \n",
    "                      if os.path.isdir(os.path.join(model_base, d))])\n",
    "    print(f\"âœ… Sections found: {sections}\\n\")\n",
    "    \n",
    "    total_trainers = 0\n",
    "    for section in sections[:3]:  # Check first 3\n",
    "        section_path = os.path.join(model_base, section)\n",
    "        try:\n",
    "            actions = os.listdir(section_path)\n",
    "            print(f\"Section {section}:\")\n",
    "            \n",
    "            for action in sorted(actions)[:3]:  # Check first 3 actions\n",
    "                action_path = os.path.join(section_path, action)\n",
    "                trainers = glob.glob(f\"{action_path}/*_trainer_*.pkl\")\n",
    "                total_trainers += len(trainers)\n",
    "                print(f\"  âœ… {action}: {len(trainers)} fold models\")\n",
    "            \n",
    "            if len(actions) > 3:\n",
    "                print(f\"  ... and {len(actions)-3} more actions\")\n",
    "        except:\n",
    "            pass\n",
    "        print()\n",
    "    \n",
    "    print(f\"ðŸ“Š Total trainer files: {total_trainers}\")\n",
    "    \n",
    "    # Check thresholds\n",
    "    thresh_file = f\"{model_base}/thresholds.pkl\"\n",
    "    if os.path.exists(thresh_file):\n",
    "        thresholds_data = joblib.load(thresh_file)\n",
    "        print(f\"âœ… Thresholds file loaded\")\n",
    "        print(f\"   Keys: {list(thresholds_data.keys())}\")\n",
    "    else:\n",
    "        print(\"âŒ thresholds.pkl NOT FOUND!\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ MODEL PATH DOES NOT EXIST!\")\n",
    "    print(f\"\\nAvailable inputs:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"  - {item}\")\n",
    "# ```\n",
    "\n",
    "# **Expected output:**\n",
    "# ```\n",
    "# ðŸ” Verifying model path...\n",
    "\n",
    "# Model path: /kaggle/input/mabe-xgboost-upgraded-models/xgboost\n",
    "# Exists: True\n",
    "\n",
    "# âœ… Sections found: ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Section 1:\n",
    "#   âœ… approach: 3 fold models\n",
    "#   âœ… attack: 3 fold models\n",
    "#   âœ… avoid: 3 fold models\n",
    "#   ... and 4 more actions\n",
    "\n",
    "# ðŸ“Š Total trainer files: 135\n",
    "# âœ… Thresholds file loaded\n",
    "#    Keys: ['single', 'pair']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9776b05",
   "metadata": {
    "papermill": {
     "duration": 0.006344,
     "end_time": "2025-12-05T15:47:14.340521",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.334177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e436f5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.355144Z",
     "iopub.status.busy": "2025-12-05T15:47:14.354375Z",
     "iopub.status.idle": "2025-12-05T15:47:14.465644Z",
     "shell.execute_reply": "2025-12-05T15:47:14.464959Z"
    },
    "papermill": {
     "duration": 0.120027,
     "end_time": "2025-12-05T15:47:14.466895",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.346868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "train['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n",
    "train_without_mabe22 = train.query(\"~lab_id.str.startswith('MABe22_')\")\n",
    "\n",
    "test = pd.read_csv(CFG.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caac7f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.481458Z",
     "iopub.status.busy": "2025-12-05T15:47:14.481214Z",
     "iopub.status.idle": "2025-12-05T15:47:14.486769Z",
     "shell.execute_reply": "2025-12-05T15:47:14.486201Z"
    },
    "papermill": {
     "duration": 0.01383,
     "end_time": "2025-12-05T15:47:14.487914",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.474084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cb6a3",
   "metadata": {
    "papermill": {
     "duration": 0.00647,
     "end_time": "2025-12-05T15:47:14.501895",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.495425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating solution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a56067f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.516710Z",
     "iopub.status.busy": "2025-12-05T15:47:14.515925Z",
     "iopub.status.idle": "2025-12-05T15:47:14.521810Z",
     "shell.execute_reply": "2025-12-05T15:47:14.521098Z"
    },
    "papermill": {
     "duration": 0.014509,
     "end_time": "2025-12-05T15:47:14.522964",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.508455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_solution_df(dataset):\n",
    "    solution = []\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    \n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'): \n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "        path = f\"{CFG.train_annotation_path}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            annot = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "        annot['lab_id'] = lab_id\n",
    "        annot['video_id'] = video_id\n",
    "        annot['behaviors_labeled'] = row['behaviors_labeled']\n",
    "        annot['target_id'] = np.where(annot.target_id != annot.agent_id, annot['target_id'].apply(lambda s: f\"mouse{s}\"), 'self')\n",
    "        annot['agent_id'] = annot['agent_id'].apply(lambda s: f\"mouse{s}\")\n",
    "        solution.append(annot)\n",
    "    \n",
    "    solution = pd.concat(solution)\n",
    "    \n",
    "    return solution\n",
    "\n",
    "if CFG.mode == 'validate':\n",
    "    solution = create_solution_df(train_without_mabe22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafe9a7",
   "metadata": {
    "papermill": {
     "duration": 0.006285,
     "end_time": "2025-12-05T15:47:14.536468",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.530183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac1ec45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.550340Z",
     "iopub.status.busy": "2025-12-05T15:47:14.550087Z",
     "iopub.status.idle": "2025-12-05T15:47:14.561133Z",
     "shell.execute_reply": "2025-12-05T15:47:14.560463Z"
    },
    "papermill": {
     "duration": 0.019314,
     "end_time": "2025-12-05T15:47:14.562260",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.542946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_body_parts =  [\n",
    "    'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "]\n",
    "\n",
    "def generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "        \n",
    "    for _, row in dataset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "        if lab_id.startswith('MABe22') or type(row.behaviors_labeled) != str: \n",
    "            continue\n",
    "        \n",
    "        video_id = row.video_id\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n",
    "        \n",
    "        del vid\n",
    "        gc.collect()\n",
    "        \n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        pvid /= row.pix_per_cm_approx\n",
    "\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "        \n",
    "        if traintest == 'train':\n",
    "            try:\n",
    "                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str[-1])\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                    single_mouse_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': mouse_id_str,\n",
    "                        'target_id': 'self',\n",
    "                        'video_frame': single_mouse.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n",
    "                    else:\n",
    "                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2): # int8\n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "                    mouse_pair_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': agent_str,\n",
    "                        'target_id': target_str,\n",
    "                        'video_frame': mouse_pair.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n",
    "                    else:\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f5b53",
   "metadata": {
    "papermill": {
     "duration": 0.006756,
     "end_time": "2025-12-05T15:47:14.576660",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.569904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforming coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9843bd12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.591904Z",
     "iopub.status.busy": "2025-12-05T15:47:14.591649Z",
     "iopub.status.idle": "2025-12-05T15:47:14.610665Z",
     "shell.execute_reply": "2025-12-05T15:47:14.610017Z"
    },
    "papermill": {
     "duration": 0.02793,
     "end_time": "2025-12-05T15:47:14.611737",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.583807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_rolling(series, window, func, min_periods=None):\n",
    "    if min_periods is None:\n",
    "        min_periods = max(1, window // 4)\n",
    "    return series.rolling(window, min_periods=min_periods, center=True).apply(func, raw=True)\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n",
    "        return float(meta_df['frames_per_second'].iloc[0])\n",
    "    vid = meta_df['video_id'].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))\n",
    "\n",
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    acc_x = vel_x.diff()\n",
    "    acc_y = vel_y.diff()\n",
    "\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n",
    "\n",
    "    for w in [25, 50, 75]:\n",
    "        ws = _scale(w, fps)\n",
    "        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n",
    "\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n",
    "        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "\n",
    "    try:\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        for window in [20, 40, 60, 80]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    X[f's{state}_{window}'] = (\n",
    "                        (speed_states == state).astype(float)\n",
    "                        .rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "                    )\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "\n",
    "    for span in [30, 60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)  # cm/s\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    approach = -rel_dist.diff()\n",
    "    chase = approach * B_lead\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c729b92",
   "metadata": {
    "papermill": {
     "duration": 0.006296,
     "end_time": "2025-12-05T15:47:14.624944",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.618648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here is new testing upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b9c9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.638969Z",
     "iopub.status.busy": "2025-12-05T15:47:14.638685Z",
     "iopub.status.idle": "2025-12-05T15:47:14.643476Z",
     "shell.execute_reply": "2025-12-05T15:47:14.642892Z"
    },
    "papermill": {
     "duration": 0.013413,
     "end_time": "2025-12-05T15:47:14.644861",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.631448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# them vao transform single\n",
    "def add_acceleration_features(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    Gia tá»‘c lÃ  chá»‰ sá»‘ quan trá»ng cho attack/chase behaviors\n",
    "    \"\"\"\n",
    "    vel_x = center_x.diff() * fps\n",
    "    vel_y = center_y.diff() * fps\n",
    "    speed = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    \n",
    "    # Acceleration magnitude\n",
    "    acc_x = vel_x.diff() * fps\n",
    "    acc_y = vel_y.diff() * fps\n",
    "    acc_mag = np.sqrt(acc_x**2 + acc_y**2)\n",
    "    \n",
    "    # Smoothed acceleration (loáº¡i nhiá»…u)\n",
    "    w = _scale(10, fps)\n",
    "    X['acc_smooth'] = acc_mag.rolling(w, min_periods=1).mean()\n",
    "    \n",
    "    # Jerk (tá»‘c Ä‘á»™ thay Ä‘á»•i gia tá»‘c - quan trá»ng cho \"sudden movements\")\n",
    "    jerk = acc_mag.diff()\n",
    "    X['jerk'] = jerk.rolling(w, min_periods=1).mean()\n",
    "    \n",
    "    # Acceleration vs speed ratio (phÃ¢n biá»‡t \"tÄƒng tá»‘c\" vs \"cháº¡y Ä‘á»u\")\n",
    "    X['acc_speed_ratio'] = acc_mag / (speed + 1e-6)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c863df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.660757Z",
     "iopub.status.busy": "2025-12-05T15:47:14.660511Z",
     "iopub.status.idle": "2025-12-05T15:47:14.670920Z",
     "shell.execute_reply": "2025-12-05T15:47:14.670102Z"
    },
    "papermill": {
     "duration": 0.019585,
     "end_time": "2025-12-05T15:47:14.672131",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.652546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pair-Specific Social Features\n",
    "def add_social_dynamics(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"\n",
    "    Social dynamics - MEMORY OPTIMIZED VERSION\n",
    "    \"\"\"\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "    \n",
    "    dx = mouse_pair['B']['body_center']['x'] - mouse_pair['A']['body_center']['x']\n",
    "    dy = mouse_pair['B']['body_center']['y'] - mouse_pair['A']['body_center']['y']\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    \n",
    "    w30 = _scale(30, fps)\n",
    "    \n",
    "    # ===== 1. APPROACH/RETREAT (3 features thay vÃ¬ 10) =====\n",
    "    dist_change = -dist.diff() * fps\n",
    "    X['approach_speed'] = dist_change\n",
    "    \n",
    "    is_approaching = (dist_change > 0).astype(float)\n",
    "    X['approach_ratio'] = is_approaching.rolling(w30, min_periods=1).mean()\n",
    "    \n",
    "    X['dist_variance'] = dist.rolling(w30, min_periods=1).var()\n",
    "    \n",
    "    # ===== 2. ORIENTATION (CHá»ˆ GIá»® QUAN TRá»ŒNG NHáº¤T - 4 features) =====\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        A_angle = np.arctan2(\n",
    "            mouse_pair['A']['nose']['y'] - mouse_pair['A']['body_center']['y'],\n",
    "            mouse_pair['A']['nose']['x'] - mouse_pair['A']['body_center']['x']\n",
    "        )\n",
    "        B_angle = np.arctan2(\n",
    "            mouse_pair['B']['nose']['y'] - mouse_pair['B']['body_center']['y'],\n",
    "            mouse_pair['B']['nose']['x'] - mouse_pair['B']['body_center']['x']\n",
    "        )\n",
    "        \n",
    "        vec_AB = np.arctan2(dy, dx)\n",
    "        \n",
    "        # A facing B (smoothed)\n",
    "        A_facing_B = np.cos(A_angle - vec_AB)\n",
    "        X['A_facing_B_avg'] = A_facing_B.rolling(_scale(15, fps), min_periods=1).mean()\n",
    "        \n",
    "        # Mutual facing (binary)\n",
    "        B_facing_A = np.cos(B_angle - vec_AB + np.pi)\n",
    "        X['mutual_facing'] = ((A_facing_B > 0.7) & (B_facing_A > 0.7)).astype(float)\n",
    "        \n",
    "        # Alignment\n",
    "        alignment = np.cos(A_angle - B_angle)\n",
    "        X['alignment'] = alignment.rolling(w30, min_periods=1).mean()\n",
    "    \n",
    "    # ===== 3. SPEED DYNAMICS (3 features thay vÃ¬ 8) =====\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "    \n",
    "    A_speed = np.sqrt(A_vx**2 + A_vy**2) * fps\n",
    "    B_speed = np.sqrt(B_vx**2 + B_vy**2) * fps\n",
    "    \n",
    "    X['speed_diff'] = np.abs(A_speed - B_speed)\n",
    "    X['speed_ratio_AB'] = A_speed / (B_speed + 1e-6)\n",
    "    \n",
    "    # Velocity correlation (CHá»ˆ 1 window)\n",
    "    if len(A_speed) >= w30:\n",
    "        X['speed_corr'] = A_speed.rolling(w30, min_periods=max(5, w30//6)).corr(B_speed)\n",
    "    \n",
    "    # ===== 4. CHASE/ESCAPE SIGNATURE (2 features thay vÃ¬ 4) =====\n",
    "    chase_sig = ((dist_change > 0) & (B_speed > A_speed * 0.8)).astype(float)\n",
    "    X['chase_score'] = chase_sig.rolling(w30, min_periods=1).mean()\n",
    "    \n",
    "    escape_sig = ((dist_change < 0) & (A_speed > B_speed * 0.8)).astype(float)\n",
    "    X['escape_score'] = escape_sig.rolling(w30, min_periods=1).mean()\n",
    "    \n",
    "    # ===== 5. PROXIMITY ZONES (CHá»ˆ 2 zones quan trá»ng - 2 features) =====\n",
    "    X['zone_close'] = (dist < 8.0).astype(float)  # Close interaction\n",
    "    X['zone_far'] = (dist >= 20.0).astype(float)  # Separate\n",
    "    \n",
    "    return X\n",
    "    # TOTAL: ~17 features thay vÃ¬ 50+ (giáº£m 66%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868c8261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.687677Z",
     "iopub.status.busy": "2025-12-05T15:47:14.686901Z",
     "iopub.status.idle": "2025-12-05T15:47:14.692941Z",
     "shell.execute_reply": "2025-12-05T15:47:14.692392Z"
    },
    "papermill": {
     "duration": 0.014724,
     "end_time": "2025-12-05T15:47:14.693955",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.679231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Temporal Context Features\n",
    "def add_temporal_context(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    Temporal context features - MEMORY OPTIMIZED VERSION\n",
    "    \"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    \n",
    "    # ===== GIáº¢M Sá» WINDOWS: CHá»ˆ GIá»® QUAN TRá»ŒNG NHáº¤T =====\n",
    "    w_recent = _scale(30, fps)\n",
    "    w_long = _scale(90, fps)  # Giáº£m tá»« 150 â†’ 90\n",
    "    \n",
    "    # Recent vs Long comparison (2 features thay vÃ¬ 7)\n",
    "    X['recent_speed'] = speed.rolling(w_recent, min_periods=max(1, w_recent//4)).mean()\n",
    "    X['long_speed'] = speed.rolling(w_long, min_periods=max(1, w_long//4)).mean()\n",
    "    \n",
    "    # Speed ratio (1 feature thay vÃ¬ 2)\n",
    "    X['speed_ratio'] = X['recent_speed'] / (X['long_speed'] + 1e-6)\n",
    "    \n",
    "    # Activity burst (2 features thay vÃ¬ 4)\n",
    "    X['is_burst'] = (X['recent_speed'] > 2 * X['long_speed']).astype(float)\n",
    "    X['burst_count'] = X['is_burst'].rolling(w_recent, min_periods=1).sum()\n",
    "    \n",
    "    # Direction stability (SIMPLIFIED - chá»‰ 1 feature)\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_diff = pd.Series(np.abs(angle.diff()), index=angle.index)\n",
    "    X['dir_change'] = angle_diff.rolling(w_recent, min_periods=1).mean()\n",
    "    \n",
    "    # Momentum magnitude (1 feature thay vÃ¬ 2)\n",
    "    momentum_x = vel_x.rolling(w_recent, min_periods=1).sum()\n",
    "    momentum_y = vel_y.rolling(w_recent, min_periods=1).sum()\n",
    "    X['momentum'] = np.sqrt(momentum_x**2 + momentum_y**2)\n",
    "    \n",
    "    return X\n",
    "    # TOTAL: 8 features thay vÃ¬ 17 (giáº£m 53%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8452016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.707982Z",
     "iopub.status.busy": "2025-12-05T15:47:14.707766Z",
     "iopub.status.idle": "2025-12-05T15:47:14.711452Z",
     "shell.execute_reply": "2025-12-05T15:47:14.710911Z"
    },
    "papermill": {
     "duration": 0.011716,
     "end_time": "2025-12-05T15:47:14.712309",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.700593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_triangle_area(ax, ay, bx, by, cx, cy):\n",
    "    \"\"\"\n",
    "    TÃ­nh diá»‡n tÃ­ch tam giÃ¡c táº¡o bá»Ÿi 3 Ä‘iá»ƒm A(ax,ay), B(bx,by), C(cx,cy)\n",
    "    Sá»­ dá»¥ng cÃ´ng thá»©c Shoelace (dÃ¢y giÃ y) cho toáº¡ Ä‘á»™.\n",
    "    \"\"\"\n",
    "    return 0.5 * np.abs(ax * (by - cy) + bx * (cy - ay) + cx * (ay - by))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddb0dd",
   "metadata": {
    "papermill": {
     "duration": 0.006369,
     "end_time": "2025-12-05T15:47:14.725297",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.718928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**A LOT OF BODY FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f429e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.739570Z",
     "iopub.status.busy": "2025-12-05T15:47:14.739270Z",
     "iopub.status.idle": "2025-12-05T15:47:14.750612Z",
     "shell.execute_reply": "2025-12-05T15:47:14.750062Z"
    },
    "papermill": {
     "duration": 0.019789,
     "end_time": "2025-12-05T15:47:14.751601",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.731812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. BODY SHAPE & POSTURE FEATURES (Quan trá»ng cho rear, groom, mount)\n",
    "# =============================================================================\n",
    "\n",
    "def add_body_shape_features(X, single_mouse, available_body_parts, fps):\n",
    "    # \"\"\"\n",
    "    # PhÃ¢n tÃ­ch hÃ¬nh dáº¡ng cÆ¡ thá»ƒ - Ráº¤T QUAN TRá»ŒNG cho behaviors\n",
    "    \n",
    "    # Key insights:\n",
    "    # - Rear: body elongated vertically (nose cao hÆ¡n tail)\n",
    "    # - Groom: body compact, head down\n",
    "    # - Mount: body above another mouse\n",
    "    # \"\"\"\n",
    "    if not all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        return X\n",
    "    \n",
    "    nose_x, nose_y = single_mouse['nose']['x'], single_mouse['nose']['y']\n",
    "    center_x, center_y = single_mouse['body_center']['x'], single_mouse['body_center']['y']\n",
    "    tail_x, tail_y = single_mouse['tail_base']['x'], single_mouse['tail_base']['y']\n",
    "    \n",
    "    # 1. Vertical vs Horizontal extent (phÃ¢n biá»‡t rear vs normal)\n",
    "    vertical_extent = np.abs(nose_y - tail_y)\n",
    "    horizontal_extent = np.abs(nose_x - tail_x)\n",
    "    X['body_aspect_ratio'] = vertical_extent / (horizontal_extent + 1e-6)\n",
    "    \n",
    "    # 2. Body orientation angle (gÃ³c nghiÃªng cÆ¡ thá»ƒ)\n",
    "    body_angle = np.arctan2(nose_y - tail_y, nose_x - tail_x)\n",
    "    X['body_angle_sin'] = np.sin(body_angle)\n",
    "    X['body_angle_cos'] = np.cos(body_angle)\n",
    "    \n",
    "    # 3. Body compactness (co cá»¥m hay duá»—i ra)\n",
    "    nose_tail_dist = np.sqrt((nose_x - tail_x)**2 + (nose_y - tail_y)**2)\n",
    "    body_perimeter = (\n",
    "        np.sqrt((nose_x - center_x)**2 + (nose_y - center_y)**2) +\n",
    "        np.sqrt((center_x - tail_x)**2 + (center_y - tail_y)**2)\n",
    "    )\n",
    "    X['body_compactness'] = nose_tail_dist / (body_perimeter + 1e-6)\n",
    "    \n",
    "    # 4. Center of mass shift (dá»‹ch chuyá»ƒn trá»ng tÃ¢m)\n",
    "    w = _scale(20, fps)\n",
    "    X['center_shift_x'] = center_x - center_x.rolling(w, min_periods=1).mean()\n",
    "    X['center_shift_y'] = center_y - center_y.rolling(w, min_periods=1).mean()\n",
    "    \n",
    "    # 5. Body stability (á»•n Ä‘á»‹nh tÆ° tháº¿)\n",
    "    w = _scale(30, fps)\n",
    "    X['aspect_stability'] = X['body_aspect_ratio'].rolling(w, min_periods=1).std()\n",
    "    X['angle_stability'] = body_angle.rolling(w, min_periods=1).std()\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def add_head_features(X, single_mouse, available_body_parts, fps):\n",
    "    # \"\"\"\n",
    "    # PhÃ¢n tÃ­ch vá»‹ trÃ­ vÃ  chuyá»ƒn Ä‘á»™ng cá»§a Ä‘áº§u\n",
    "    # Quan trá»ng cho: investigation, sniff, groom\n",
    "    # \"\"\"\n",
    "    if 'nose' not in available_body_parts or 'body_center' not in available_body_parts:\n",
    "        return X\n",
    "    \n",
    "    nose_x, nose_y = single_mouse['nose']['x'], single_mouse['nose']['y']\n",
    "    center_x, center_y = single_mouse['body_center']['x'], single_mouse['body_center']['y']\n",
    "    \n",
    "    # 1. Head extension (khoáº£ng cÃ¡ch mÅ©i - thÃ¢n)\n",
    "    head_extension = np.sqrt((nose_x - center_x)**2 + (nose_y - center_y)**2)\n",
    "    w = _scale(15, fps)\n",
    "    X['head_ext_mean'] = head_extension.rolling(w, min_periods=1).mean()\n",
    "    X['head_ext_std'] = head_extension.rolling(w, min_periods=1).std()\n",
    "    \n",
    "    # 2. Head bobbing (gáº­t Ä‘áº§u lÃªn xuá»‘ng - quan trá»ng cho investigation)\n",
    "    head_vertical_vel = nose_y.diff() * fps\n",
    "    w = _scale(10, fps)\n",
    "    X['head_bob_freq'] = head_vertical_vel.rolling(w, min_periods=1).apply(\n",
    "        lambda x: np.sum(np.diff(np.sign(x)) != 0) if len(x) > 1 else 0,\n",
    "        raw=True\n",
    "    )\n",
    "    \n",
    "    # 3. Head scanning (quÃ©t ngang - quan trá»ng cho investigation)\n",
    "    head_horizontal_vel = nose_x.diff() * fps\n",
    "    X['head_scan_speed'] = np.abs(head_horizontal_vel).rolling(w, min_periods=1).mean()\n",
    "    \n",
    "    # 4. Nose trajectory complexity (Ä‘á»™ phá»©c táº¡p quá»¹ Ä‘áº¡o mÅ©i)\n",
    "    w = _scale(30, fps)\n",
    "    nose_displacement = np.sqrt(nose_x.diff()**2 + nose_y.diff()**2).rolling(w, min_periods=1).sum()\n",
    "    nose_direct_distance = np.sqrt(\n",
    "        (nose_x - nose_x.shift(w))**2 + \n",
    "        (nose_y - nose_y.shift(w))**2\n",
    "    )\n",
    "    X['nose_path_complexity'] = nose_displacement / (nose_direct_distance + 1e-6)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def add_tail_features(X, single_mouse, available_body_parts, fps):\n",
    "    # \"\"\"\n",
    "    # PhÃ¢n tÃ­ch chuyá»ƒn Ä‘á»™ng Ä‘uÃ´i\n",
    "    # Quan trá»ng cho: anxiety, arousal states\n",
    "    # \"\"\"\n",
    "    if 'tail_base' not in available_body_parts:\n",
    "        return X\n",
    "    \n",
    "    tail_x, tail_y = single_mouse['tail_base']['x'], single_mouse['tail_base']['y']\n",
    "    \n",
    "    # 1. Tail movement speed\n",
    "    tail_speed = np.sqrt(tail_x.diff()**2 + tail_y.diff()**2) * fps\n",
    "    w = _scale(20, fps)\n",
    "    X['tail_speed_mean'] = tail_speed.rolling(w, min_periods=1).mean()\n",
    "    X['tail_speed_std'] = tail_speed.rolling(w, min_periods=1).std()\n",
    "    \n",
    "    # 2. Tail elevation change (Ä‘uÃ´i giÆ¡ lÃªn/háº¡ xuá»‘ng)\n",
    "    if 'body_center' in available_body_parts:\n",
    "        center_y = single_mouse['body_center']['y']\n",
    "        tail_elevation = tail_y - center_y\n",
    "        X['tail_elevation'] = tail_elevation.rolling(w, min_periods=1).mean()\n",
    "        X['tail_elev_change'] = tail_elevation.diff().rolling(w, min_periods=1).std()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fa30e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.766160Z",
     "iopub.status.busy": "2025-12-05T15:47:14.765917Z",
     "iopub.status.idle": "2025-12-05T15:47:14.781266Z",
     "shell.execute_reply": "2025-12-05T15:47:14.780629Z"
    },
    "papermill": {
     "duration": 0.023746,
     "end_time": "2025-12-05T15:47:14.782247",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.758501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "\n",
    "    # ========== PHáº¦N 1: FEATURES CÅ¨ (GIá»® NGUYÃŠN) ==========\n",
    "    X = pd.DataFrame({\n",
    "        f\"{p1}+{p2}\": np.square(single_mouse[p1] - single_mouse[p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n",
    "        if p1 in available_body_parts and p2 in available_body_parts\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n",
    "\n",
    "    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        lag = _scale(10, fps)\n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_lf': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n",
    "            'sp_rt': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n",
    "            'sp_lf2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "            'sp_rt2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose'] - single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "        X['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (\n",
    "            np.sqrt(v1['x']**2 + v1['y']**2) * np.sqrt(v2['x']**2 + v2['y']**2) + 1e-6)\n",
    "\n",
    "        nose_x, nose_y = single_mouse['nose']['x'], single_mouse['nose']['y']\n",
    "        center_x, center_y = single_mouse['body_center']['x'], single_mouse['body_center']['y']\n",
    "        tail_x, tail_y = single_mouse['tail_base']['x'], single_mouse['tail_base']['y']\n",
    "        \n",
    "        X['body_area'] = calculate_triangle_area(nose_x, nose_y, center_x, center_y, tail_x, tail_y)\n",
    "        X['body_area_change'] = X['body_area'].diff()\n",
    "        w = _scale(10, fps)\n",
    "        X['body_area_mean_10'] = X['body_area'].rolling(w, min_periods=1).mean()\n",
    "\n",
    "    if 'body_center' in available_body_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'cx_m{w}'] = cx.rolling(ws, **roll).mean()\n",
    "            X[f'cy_m{w}'] = cy.rolling(ws, **roll).mean()\n",
    "            X[f'cx_s{w}'] = cx.rolling(ws, **roll).std()\n",
    "            X[f'cy_s{w}'] = cy.rolling(ws, **roll).std()\n",
    "            X[f'x_rng{w}'] = cx.rolling(ws, **roll).max() - cx.rolling(ws, **roll).min()\n",
    "            X[f'y_rng{w}'] = cy.rolling(ws, **roll).max() - cy.rolling(ws, **roll).min()\n",
    "            X[f'disp{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).sum()**2 +\n",
    "                                     cy.diff().rolling(ws, min_periods=1).sum()**2)\n",
    "            X[f'act{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).var() +\n",
    "                                   cy.diff().rolling(ws, min_periods=1).var())\n",
    "\n",
    "        X = add_curvature_features(X, cx, cy, fps)\n",
    "        X = add_multiscale_features(X, cx, cy, fps)\n",
    "        X = add_state_features(X, cx, cy, fps)\n",
    "        X = add_longrange_features(X, cx, cy, fps)\n",
    "        X = add_temporal_context(X, cx, cy, fps)\n",
    "    \n",
    "    # ðŸ”¥ FEATURES Má»šI CHO BODY PARTS\n",
    "    X = add_body_shape_features(X, single_mouse, available_body_parts, fps)\n",
    "    X = add_head_features(X, single_mouse, available_body_parts, fps)\n",
    "    X = add_tail_features(X, single_mouse, available_body_parts, fps)\n",
    "    \n",
    "    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n",
    "        nt_dist = np.sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n",
    "                          (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nt_lg{lag}'] = nt_dist.shift(l)\n",
    "            X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 +\n",
    "                        (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n",
    "        for off in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "        w = _scale(30, fps)\n",
    "        X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "        # FEATURE Má»šI 3: Acceleration\n",
    "    if 'body_center' in available_body_parts:\n",
    "        vel_x = single_mouse['body_center']['x'].diff() * fps\n",
    "        vel_y = single_mouse['body_center']['y'].diff() * fps\n",
    "        acc_x = vel_x.diff() * fps\n",
    "        acc_y = vel_y.diff() * fps\n",
    "        X['acceleration'] = np.sqrt(acc_x**2 + acc_y**2)\n",
    "        w = _scale(15, fps)\n",
    "        X['acc_smooth'] = X['acceleration'].rolling(w, min_periods=1).mean()\n",
    "\n",
    "        # FEATURE Má»šI: Jerk (Ä‘áº¡o hÃ m báº­c 3 - Ä‘á»™ giáº­t)\n",
    "    if 'body_center' in available_body_parts:\n",
    "        vel_x = single_mouse['body_center']['x'].diff() * fps\n",
    "        vel_y = single_mouse['body_center']['y'].diff() * fps\n",
    "        acc_x = vel_x.diff() * fps\n",
    "        acc_y = vel_y.diff() * fps\n",
    "        jerk_x = acc_x.diff() * fps\n",
    "        jerk_y = acc_y.diff() * fps\n",
    "        X['jerk'] = np.sqrt(jerk_x**2 + jerk_y**2)\n",
    "        w = _scale(10, fps)\n",
    "        X['jerk_smooth'] = X['jerk'].rolling(w, min_periods=1).mean()\n",
    "\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abacb822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.797141Z",
     "iopub.status.busy": "2025-12-05T15:47:14.796889Z",
     "iopub.status.idle": "2025-12-05T15:47:14.817525Z",
     "shell.execute_reply": "2025-12-05T15:47:14.816886Z"
    },
    "papermill": {
     "duration": 0.029147,
     "end_time": "2025-12-05T15:47:14.818508",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.789361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    avail_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    avail_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "\n",
    "    X = pd.DataFrame({\n",
    "        f\"12+{p1}+{p2}\": np.square(mouse_pair['A'][p1] - mouse_pair['B'][p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n",
    "        if p1 in avail_A and p2 in avail_B\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shA = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shB = mouse_pair['B']['ear_left'].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_A': np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False),\n",
    "            'sp_AB': np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "            'sp_B': np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n",
    "            np.sqrt(dir_A['x']**2 + dir_A['y']**2) * np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n",
    "\n",
    "    if all(p in avail_A for p in ['nose']) and all(p in avail_B for p in ['nose']):\n",
    "        cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n",
    "        lag = _scale(10, fps)\n",
    "        shA_n = mouse_pair['A']['nose'].shift(lag)\n",
    "        shB_n = mouse_pair['B']['nose'].shift(lag)\n",
    "        past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n",
    "        X['appr'] = cur - past\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd = np.sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "                     (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "        X['v_cls'] = (cd < 5.0).astype(float)\n",
    "        X['cls']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n",
    "        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "        X['far']   = (cd >= 30.0).astype(float)\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd_full = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'd_m{w}']  = cd_full.rolling(ws, **roll).mean()\n",
    "            X[f'd_s{w}']  = cd_full.rolling(ws, **roll).std()\n",
    "            X[f'd_mn{w}'] = cd_full.rolling(ws, **roll).min()\n",
    "            X[f'd_mx{w}'] = cd_full.rolling(ws, **roll).max()\n",
    "\n",
    "            d_var = cd_full.rolling(ws, **roll).var()\n",
    "            X[f'int{w}'] = 1 / (1 + d_var)\n",
    "\n",
    "            Axd = mouse_pair['A']['body_center']['x'].diff()\n",
    "            Ayd = mouse_pair['A']['body_center']['y'].diff()\n",
    "            Bxd = mouse_pair['B']['body_center']['x'].diff()\n",
    "            Byd = mouse_pair['B']['body_center']['y'].diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{w}'] = coord.rolling(ws, **roll).mean()\n",
    "            X[f'co_s{w}'] = coord.rolling(ws, **roll).std()\n",
    "\n",
    "        # ðŸ”¥ NEW: Pair-specific social dynamics (LEVEL 2 upgrade)\n",
    "        X = add_social_dynamics(X, mouse_pair, avail_A, avail_B, fps)\n",
    "\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn = np.sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n",
    "                     (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}']  = nn.shift(l)\n",
    "            X[f'nn_ch{lag}']  = nn - nn.shift(l)\n",
    "            is_cl = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}']  = is_cl.rolling(l, min_periods=1).mean()\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        Avx = mouse_pair['A']['body_center']['x'].diff()\n",
    "        Avy = mouse_pair['A']['body_center']['y'].diff()\n",
    "        Bvx = mouse_pair['B']['body_center']['x'].diff()\n",
    "        Bvy = mouse_pair['B']['body_center']['y'].diff()\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "\n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'va_{off}'] = val.shift(-o)\n",
    "\n",
    "        w = _scale(30, fps)\n",
    "        X['int_con'] = cd_full.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (cd_full.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "        X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "        X = add_social_dynamics(X, mouse_pair, avail_A, avail_B, fps)  # â† THÃŠM DÃ’NG NÃ€Y\n",
    "        # FEATURE Má»šI 1 & 2: Speed Product + Nose-Tail\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        A_vx = mouse_pair['A']['body_center']['x'].diff() * fps\n",
    "        A_vy = mouse_pair['A']['body_center']['y'].diff() * fps\n",
    "        A_speed = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_vx = mouse_pair['B']['body_center']['x'].diff() * fps\n",
    "        B_vy = mouse_pair['B']['body_center']['y'].diff() * fps\n",
    "        B_speed = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X['speed_product'] = A_speed * B_speed\n",
    "    \n",
    "    if 'nose' in avail_A and 'tail_base' in avail_B:\n",
    "        X['A_nose_B_tail'] = np.sqrt(\n",
    "            (mouse_pair['A']['nose']['x'] - mouse_pair['B']['tail_base']['x'])**2 +\n",
    "            (mouse_pair['A']['nose']['y'] - mouse_pair['B']['tail_base']['y'])**2\n",
    "        )\n",
    "    if 'nose' in avail_B and 'tail_base' in avail_A:\n",
    "        X['B_nose_A_tail'] = np.sqrt(\n",
    "            (mouse_pair['B']['nose']['x'] - mouse_pair['A']['tail_base']['x'])**2 +\n",
    "            (mouse_pair['B']['nose']['y'] - mouse_pair['A']['tail_base']['y'])**2\n",
    "        )\n",
    "\n",
    "        # FEATURE Má»šI: Relative Speed (tá»‘c Ä‘á»™ tiáº¿n Ä‘áº¿n target)\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        # Speed toward/away from target\n",
    "        dx = mouse_pair['B']['body_center']['x'] - mouse_pair['A']['body_center']['x']\n",
    "        dy = mouse_pair['B']['body_center']['y'] - mouse_pair['A']['body_center']['y']\n",
    "        dist = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # Radial velocity (positive = approaching, negative = retreating)\n",
    "        X['radial_velocity'] = -dist.diff() * fps\n",
    "        w = _scale(15, fps)\n",
    "        X['radial_vel_smooth'] = X['radial_velocity'].rolling(w, min_periods=1).mean()\n",
    "\n",
    "        # FEATURE Má»šI: Body Orientation Difference\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        # A's body angle\n",
    "        A_angle = np.arctan2(\n",
    "            mouse_pair['A']['nose']['y'] - mouse_pair['A']['tail_base']['y'],\n",
    "            mouse_pair['A']['nose']['x'] - mouse_pair['A']['tail_base']['x']\n",
    "        )\n",
    "        # B's body angle\n",
    "        B_angle = np.arctan2(\n",
    "            mouse_pair['B']['nose']['y'] - mouse_pair['B']['tail_base']['y'],\n",
    "            mouse_pair['B']['nose']['x'] - mouse_pair['B']['tail_base']['x']\n",
    "        )\n",
    "        # Angle difference (quan trá»ng cho mount detection)\n",
    "        X['body_angle_diff'] = np.abs(A_angle - B_angle)\n",
    "        X['body_parallel'] = (np.cos(A_angle - B_angle) > 0.7).astype(float)\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ba2c5",
   "metadata": {
    "papermill": {
     "duration": 0.006472,
     "end_time": "2025-12-05T15:47:14.831999",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.825527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training, validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b624cda6",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.846422Z",
     "iopub.status.busy": "2025-12-05T15:47:14.846155Z",
     "iopub.status.idle": "2025-12-05T15:47:14.855511Z",
     "shell.execute_reply": "2025-12-05T15:47:14.854851Z"
    },
    "papermill": {
     "duration": 0.018031,
     "end_time": "2025-12-05T15:47:14.856551",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.838520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def robustify(submission, dataset, traintest, traintest_directory=None):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "\n",
    "    old_submission = submission.copy()\n",
    "    submission = submission[submission.start_frame < submission.stop_frame]\n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped frames with start >= stop\")\n",
    "    \n",
    "    old_submission = submission.copy()\n",
    "    group_list = []\n",
    "    for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n",
    "        group = group.sort_values('start_frame')\n",
    "        mask = np.ones(len(group), dtype=bool)\n",
    "        last_stop_frame = 0\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            if row['start_frame'] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                last_stop_frame = row['stop_frame']\n",
    "        group_list.append(group[mask])\n",
    "        \n",
    "    submission = pd.concat(group_list)\n",
    "    \n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped duplicate frames\")\n",
    "        \n",
    "    s_list = []\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "        \n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "\n",
    "        print(f\"Video {video_id} has no predictions.\")\n",
    "        \n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "    \n",
    "        vid_behaviors = json.loads(row['behaviors_labeled'])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "    \n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "    \n",
    "        for (agent, target), actions in vid_behaviors.groupby(['agent', 'target']):\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "                s_list.append((video_id, agent, target, action_row['action'], batch_start, batch_stop))\n",
    "\n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n",
    "        ])\n",
    "        print(\"ERROR: Filled empty videos\")\n",
    "\n",
    "    submission = submission.reset_index(drop=True)\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1289be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.871530Z",
     "iopub.status.busy": "2025-12-05T15:47:14.871292Z",
     "iopub.status.idle": "2025-12-05T15:47:14.877982Z",
     "shell.execute_reply": "2025-12-05T15:47:14.877393Z"
    },
    "papermill": {
     "duration": 0.015149,
     "end_time": "2025-12-05T15:47:14.878989",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.863840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_multiclass(pred, meta, thresholds):\n",
    "    ama = np.argmax(pred.values, axis=1)\n",
    "    max_proba = pred.max(axis=1).values\n",
    "\n",
    "    threshold_array = np.array([thresholds.get(col, 0.27) for col in pred.columns])\n",
    "    action_thresholds = threshold_array[ama]\n",
    "\n",
    "    ama = np.where(max_proba >= action_thresholds, ama, -1)\n",
    "    ama = pd.Series(ama, index=meta.video_frame)\n",
    "    \n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    \n",
    "    mask = ama_changes.values >= 0\n",
    "    mask[-1] = False\n",
    "    \n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes['video_id'][mask].values,\n",
    "        'agent_id': meta_changes['agent_id'][mask].values,\n",
    "        'target_id': meta_changes['target_id'][mask].values,\n",
    "        'action': pred.columns[ama_changes[mask].values],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    stop_video_id = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        if stop_video_id[i] != video_id or stop_agent_id[i] != agent_id or stop_target_id[i] != target_id:\n",
    "            new_stop_frame = meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "\n",
    "    return submission_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3a7081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.893885Z",
     "iopub.status.busy": "2025-12-05T15:47:14.893608Z",
     "iopub.status.idle": "2025-12-05T15:47:14.897829Z",
     "shell.execute_reply": "2025-12-05T15:47:14.897265Z"
    },
    "papermill": {
     "duration": 0.012485,
     "end_time": "2025-12-05T15:47:14.898762",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.886277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tune_threshold(oof_action, y_action):\n",
    "    def objective(trial):\n",
    "        threshold = trial.suggest_float(\"threshold\", 0, 1, step=0.01)\n",
    "        return f1_score(y_action, (oof_action >= threshold), zero_division=0)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "    return study.best_params[\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab4ed1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.913296Z",
     "iopub.status.busy": "2025-12-05T15:47:14.913066Z",
     "iopub.status.idle": "2025-12-05T15:47:14.921236Z",
     "shell.execute_reply": "2025-12-05T15:47:14.920622Z"
    },
    "papermill": {
     "duration": 0.016509,
     "end_time": "2025-12-05T15:47:14.922206",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.905697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate_classifier(X, label, meta, body_parts_tracked_str, section):\n",
    "    oof = pd.DataFrame(index=meta.video_frame)\n",
    "    \n",
    "    f1_list = []\n",
    "    submission_list = []\n",
    "    thresholds = {}\n",
    "    \n",
    "    for action in label.columns:\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        X_action = X[action_mask]\n",
    "        groups_action = meta.video_id[action_mask]\n",
    "        \n",
    "        if len(np.unique(groups_action)) < CFG.n_splits:\n",
    "            continue\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "                    \n",
    "                    trainer = Trainer(\n",
    "                        estimator=clone(CFG.model),\n",
    "                        cv=CFG.cv,\n",
    "                        cv_args={\"groups\": groups_action},\n",
    "                        metric=f1_score,\n",
    "                        task=\"binary\",\n",
    "                        verbose=False,\n",
    "                        save=True,\n",
    "                        save_path=f\"{CFG.model_name}/{section}/{action}\"\n",
    "                    )\n",
    "\n",
    "                    trainer.fit(X_action, y_action)\n",
    "                    oof_action = trainer.oof_preds\n",
    "\n",
    "                    # Ãp dá»¥ng Gaussian Rolling Average Ä‘á»ƒ lÃ m mÆ°á»£t xÃ¡c suáº¥t\n",
    "                    # Window size cÃ ng lá»›n thÃ¬ cÃ ng mÆ°á»£t, nhÆ°ng coi chá»«ng máº¥t cÃ¡c hÃ nh Ä‘á»™ng nhanh\n",
    "                    # Vá»›i 30fps, window=15 tá»©c lÃ  0.5 giÃ¢y\n",
    "                    oof_series = pd.Series(oof_action)\n",
    "                    # LÃ m mÆ°á»£t trung bÃ¬nh\n",
    "                    oof_smooth = oof_series.rolling(window=15, center=True, min_periods=1).mean()\n",
    "                    # GÃ¡n ngÆ°á»£c láº¡i\n",
    "                    oof_action = oof_smooth.values\n",
    "                    # -------------------------\n",
    "\n",
    "                    threshold = tune_threshold(oof_action, y_action)\n",
    "                    thresholds[action] = threshold\n",
    "            \n",
    "                    f1 = f1_score(y_action, (oof_action >= threshold), zero_division=0)\n",
    "                    f1_list.append((body_parts_tracked_str, action, f1))\n",
    "                    \n",
    "                    joblib.dump(oof_action, f\"{CFG.model_name}/{section}/{action}/oof_pred_probs.pkl\")\n",
    "                    joblib.dump(threshold, f\"{CFG.model_name}/{section}/{action}/threshold.pkl\")\n",
    "                    \n",
    "                    print(f\"\\tF1: {f1:.4f} ({threshold:.2f}) Section: {section} Action: {action}\")\n",
    "    \n",
    "                    del trainer\n",
    "                    gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                oof_action = np.zeros(len(y_action))\n",
    "                print(f\"\\tF1: 0.0000 (0.00) Section: {section} Action: {action}\")\n",
    "        \n",
    "        else:\n",
    "            oof_action = np.zeros(len(y_action))\n",
    "            print(f\"\\tF1: 0.0000 (0.00) Section: {section} Action: {action}\")\n",
    "        \n",
    "        oof_column = np.zeros(len(label))\n",
    "        oof_column[action_mask] = oof_action\n",
    "        oof[action] = oof_column\n",
    "\n",
    "        del oof_action, action_mask, X_action, y_action, groups_action\n",
    "        gc.collect()\n",
    "\n",
    "    submission_part = predict_multiclass(oof, meta, thresholds)\n",
    "    submission_list.append(submission_part)\n",
    "    \n",
    "    return submission_list, f1_list, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fad1aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.937112Z",
     "iopub.status.busy": "2025-12-05T15:47:14.936856Z",
     "iopub.status.idle": "2025-12-05T15:47:14.945290Z",
     "shell.execute_reply": "2025-12-05T15:47:14.944646Z"
    },
    "papermill": {
     "duration": 0.016812,
     "end_time": "2025-12-05T15:47:14.946279",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.929467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(body_parts_tracked_str, switch_tr, section, thresholds):    \n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "        \n",
    "    test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "    generator = generate_mouse_data(\n",
    "        test_subset, \n",
    "        'test',\n",
    "        generate_single=(switch_tr == 'single'), \n",
    "        generate_pair=(switch_tr == 'pair')\n",
    "    )\n",
    "\n",
    "    fps_lookup = (\n",
    "        test_subset[['video_id', 'frames_per_second']]\n",
    "        .drop_duplicates('video_id')\n",
    "        .set_index('video_id')['frames_per_second']\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    submission_list = []\n",
    "    for switch_te, data_te, meta_te, actions_te in generator:\n",
    "        assert switch_te == switch_tr\n",
    "        try:\n",
    "            fps_i = _fps_from_meta(meta_te, fps_lookup, default_fps=30.0)\n",
    "            \n",
    "            if switch_te == 'single':\n",
    "                X_te = transform_single(data_te, body_parts_tracked, fps_i)\n",
    "            else:\n",
    "                X_te = transform_pair(data_te, body_parts_tracked, fps_i)\n",
    "            del data_te\n",
    "            gc.collect()\n",
    "    \n",
    "            pred = pd.DataFrame(index=meta_te.video_frame)\n",
    "            for action in actions_te:\n",
    "                # ===== ENSEMBLE Táº¤T Cáº¢ FOLDS (FIXED) =====\n",
    "                files = glob.glob(f\"{CFG.model_path}/{CFG.model_name}/{section}/{action}/*_trainer_*.pkl\")\n",
    "                \n",
    "                if len(files) > 0:  # â† THAY Äá»”I: tá»« \"== 1\" thÃ nh \"> 0\"\n",
    "                    print(f\"      Loading {len(files)} models for {action}\")\n",
    "                    preds = []\n",
    "                    \n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            with open(f, 'rb') as file_handle:\n",
    "                                trainer = joblib.load(file_handle)\n",
    "                                preds.append(trainer.predict(X_te))\n",
    "                                del trainer\n",
    "                        except Exception as e:\n",
    "                            print(f\"        Error loading {f}: {e}\")\n",
    "                        finally:\n",
    "                            gc.collect()\n",
    "                    \n",
    "                    if len(preds) > 0:\n",
    "                        pred[action] = np.mean(preds, axis=0)\n",
    "                        del preds\n",
    "                        gc.collect()\n",
    "                \n",
    "            del X_te\n",
    "            gc.collect()\n",
    "\n",
    "            if pred.shape[1] != 0:\n",
    "                # ===== THÃŠM SMOOTHING NHÆ¯ TRAINING =====\n",
    "                pred = pred.rolling(window=15, center=True, min_periods=1).mean()\n",
    "                \n",
    "                submission_part = predict_multiclass(pred, meta_te, thresholds)\n",
    "                submission_list.append(submission_part)\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"      KeyError: {e}\")\n",
    "            del data_te\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"      Error: {e}\")\n",
    "            gc.collect()\n",
    "            \n",
    "    return submission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561eb004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.960784Z",
     "iopub.status.busy": "2025-12-05T15:47:14.960545Z",
     "iopub.status.idle": "2025-12-05T15:47:14.964544Z",
     "shell.execute_reply": "2025-12-05T15:47:14.963974Z"
    },
    "papermill": {
     "duration": 0.012359,
     "end_time": "2025-12-05T15:47:14.965558",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.953199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == \"validate\":\n",
    "    thresholds = {\n",
    "        \"single\": {},\n",
    "        \"pair\": {}\n",
    "    }\n",
    "else:\n",
    "    thresholds = joblib.load(f\"{CFG.model_path}/{CFG.model_name}/thresholds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91d4ad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.980188Z",
     "iopub.status.busy": "2025-12-05T15:47:14.979916Z",
     "iopub.status.idle": "2025-12-05T15:47:14.983970Z",
     "shell.execute_reply": "2025-12-05T15:47:14.983262Z"
    },
    "papermill": {
     "duration": 0.012365,
     "end_time": "2025-12-05T15:47:14.984879",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.972514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test features generation cell\n",
    "# print(\"Testing feature generation...\")\n",
    "\n",
    "# test_subset = train.head(1)\n",
    "# for switch, data, meta, label in generate_mouse_data(test_subset, 'train'):\n",
    "#     fps_test = 30.0\n",
    "    \n",
    "#     if switch == 'single':\n",
    "#         X_test = transform_single(data, json.loads(test_subset.iloc[0].body_parts_tracked), fps_test)\n",
    "#         print(f\"âœ… Single features: {X_test.shape}\")\n",
    "#         print(f\"   Sample temporal features: {[c for c in X_test.columns if 'recent' in c or 'burst' in c][:5]}\")\n",
    "#         print(f\"   Total temporal features: {len([c for c in X_test.columns if any(k in c for k in ['recent', 'med_speed', 'long_speed', 'burst', 'calm', 'momentum'])])}\")\n",
    "#     else:\n",
    "#         X_test = transform_pair(data, json.loads(test_subset.iloc[0].body_parts_tracked), fps_test)\n",
    "#         print(f\"âœ… Pair features: {X_test.shape}\")\n",
    "#         print(f\"   Sample social features: {[c for c in X_test.columns if 'approach' in c or 'chase' in c][:5]}\")\n",
    "#         print(f\"   Total social features: {len([c for c in X_test.columns if any(k in c for k in ['approach', 'chase', 'escape', 'facing', 'zone'])])}\")\n",
    "    \n",
    "#     print(f\"\\n   No NaN: {X_test.isna().sum().sum() == 0}\")\n",
    "#     print(f\"   No Inf: {np.isinf(X_test.values).sum() == 0}\")\n",
    "#     break\n",
    "\n",
    "# print(\"\\nâœ… Feature generation test passed!\")\n",
    "# # ```\n",
    "\n",
    "# # **Expected output:**\n",
    "# # ```\n",
    "# # Testing feature generation...\n",
    "# # âœ… Single features: (8234, 182)\n",
    "# #    Sample temporal features: ['recent_speed_mean', 'recent_speed_std', 'recent_speed_max', 'med_speed_mean', 'med_speed_std']\n",
    "# #    Total temporal features: 17\n",
    "\n",
    "# #    No NaN: True\n",
    "# #    No Inf: True\n",
    "\n",
    "# # âœ… Feature generation test passed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f891483c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T15:47:14.999569Z",
     "iopub.status.busy": "2025-12-05T15:47:14.999338Z",
     "iopub.status.idle": "2025-12-05T16:05:47.828827Z",
     "shell.execute_reply": "2025-12-05T16:05:47.828053Z"
    },
    "papermill": {
     "duration": 1112.83832,
     "end_time": "2025-12-05T16:05:47.830408",
     "exception": false,
     "start_time": "2025-12-05T15:47:14.992088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', 'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "\n",
      "      Loading 1 models for rear\n",
      "      Loading 1 models for rear\n",
      "      Loading 1 models for rear\n",
      "      Loading 1 models for rear\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "      Loading 1 models for approach\n",
      "      Loading 1 models for attack\n",
      "      Loading 1 models for avoid\n",
      "      Loading 1 models for chase\n",
      "      Loading 1 models for chaseattack\n",
      "      Loading 1 models for submit\n",
      "\n",
      "2/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'hip_left', 'hip_right', 'lateral_left', 'lateral_right', 'nose', 'spine_1', 'spine_2', 'tail_base', 'tail_middle_1', 'tail_middle_2', 'tail_tip']\n",
      "\n",
      "\n",
      "3/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "\n",
      "\n",
      "4/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base', 'tail_tip']\n",
      "\n",
      "\n",
      "5/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base']\n",
      "\n",
      "\n",
      "6/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'nose', 'tail_base']\n",
      "\n",
      "\n",
      "7/9 Processing videos with: ['ear_left', 'ear_right', 'head', 'tail_base']\n",
      "\n",
      "\n",
      "8/9 Processing videos with: ['ear_left', 'ear_right', 'hip_left', 'hip_right', 'neck', 'nose', 'tail_base']\n",
      "\n",
      "\n",
      "9/9 Processing videos with: ['ear_left', 'ear_right', 'nose', 'tail_base', 'tail_tip']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_list = []\n",
    "submission_list = []\n",
    "\n",
    "for section in range(1, len(body_parts_tracked_list)):\n",
    "    body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "    try:\n",
    "        body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "        print(f\"{section}/{len(body_parts_tracked_list)-1} Processing videos with: {body_parts_tracked}\\n\")\n",
    "        \n",
    "        if len(body_parts_tracked) > 5:\n",
    "            body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    \n",
    "        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "        _fps_lookup = (\n",
    "            train_subset[['video_id', 'frames_per_second']]\n",
    "            .drop_duplicates('video_id')\n",
    "            .set_index('video_id')['frames_per_second']\n",
    "            .to_dict()\n",
    "        )\n",
    "        \n",
    "        single_mouse_list = []\n",
    "        single_mouse_label_list = []\n",
    "        single_mouse_meta_list = []\n",
    "        \n",
    "        mouse_pair_list = []\n",
    "        mouse_pair_label_list = []\n",
    "        mouse_pair_meta_list = []\n",
    "    \n",
    "        for switch, data, meta, label in generate_mouse_data(train_subset, 'train'):\n",
    "            if switch == 'single':\n",
    "                single_mouse_list.append(data)\n",
    "                single_mouse_meta_list.append(meta)\n",
    "                single_mouse_label_list.append(label)\n",
    "            else:\n",
    "                mouse_pair_list.append(data)\n",
    "                mouse_pair_meta_list.append(meta)\n",
    "                mouse_pair_label_list.append(label)\n",
    "            \n",
    "            del data, meta, label\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "        if len(single_mouse_list) > 0:\n",
    "            single_feats_parts = []\n",
    "            for data_i, meta_i in zip(single_mouse_list, single_mouse_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n",
    "                X_i = transform_single(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                single_feats_parts.append(X_i)\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(single_feats_parts, axis=0, ignore_index=True)\n",
    "            single_mouse_label = pd.concat(single_mouse_label_list, axis=0, ignore_index=True)\n",
    "            single_mouse_meta = pd.concat(single_mouse_meta_list, axis=0, ignore_index=True)\n",
    "            \n",
    "            del single_feats_parts, single_mouse_list, single_mouse_label_list, single_mouse_meta_list\n",
    "            gc.collect()\n",
    "\n",
    "            if CFG.mode == 'validate':\n",
    "                temp_submission_list, temp_f1_list, temp_thresholds = cross_validate_classifier(X_tr, single_mouse_label, single_mouse_meta, body_parts_tracked_str, section)\n",
    "                \n",
    "                if f\"{section}\" not in thresholds[\"single\"].keys():\n",
    "                    thresholds[\"single\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_thresholds.items():\n",
    "                    thresholds[\"single\"][f\"{section}\"][k] = v                  \n",
    "                \n",
    "                f1_list.extend(temp_f1_list)\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, temp_f1_list, temp_thresholds, X_tr\n",
    "                gc.collect()\n",
    "            else:\n",
    "                temp_submission_list = submit(body_parts_tracked_str, 'single', section, thresholds[\"single\"][f\"{section}\"])\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, X_tr\n",
    "                gc.collect()\n",
    "                \n",
    "        if len(mouse_pair_list) > 0:\n",
    "            pair_feats_parts = []\n",
    "            for data_i, meta_i in zip(mouse_pair_list, mouse_pair_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n",
    "                X_i = transform_pair(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                pair_feats_parts.append(X_i)\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(pair_feats_parts, axis=0, ignore_index=True)\n",
    "            mouse_pair_label = pd.concat(mouse_pair_label_list, axis=0, ignore_index=True)\n",
    "            mouse_pair_meta = pd.concat(mouse_pair_meta_list, axis=0, ignore_index=True)\n",
    "            \n",
    "            del pair_feats_parts, mouse_pair_list, mouse_pair_label_list, mouse_pair_meta_list\n",
    "            gc.collect()\n",
    "\n",
    "            if CFG.mode == 'validate':\n",
    "                temp_submission_list, temp_f1_list, temp_thresholds = cross_validate_classifier(X_tr, mouse_pair_label, mouse_pair_meta, body_parts_tracked_str, section)\n",
    "\n",
    "                if f\"{section}\" not in thresholds[\"pair\"].keys():\n",
    "                    thresholds[\"pair\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_thresholds.items():\n",
    "                    thresholds[\"pair\"][f\"{section}\"][k] = v  \n",
    "                    \n",
    "                f1_list.extend(temp_f1_list)\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, temp_f1_list, temp_thresholds, X_tr\n",
    "                gc.collect()\n",
    "            else:\n",
    "                temp_submission_list = submit(body_parts_tracked_str, 'pair', section, thresholds[\"pair\"][f\"{section}\"])\n",
    "                \n",
    "                submission_list.extend(temp_submission_list)\n",
    "                del temp_submission_list, X_tr\n",
    "                gc.collect()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\t{e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8448c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:05:47.854658Z",
     "iopub.status.busy": "2025-12-05T16:05:47.854167Z",
     "iopub.status.idle": "2025-12-05T16:05:47.872993Z",
     "shell.execute_reply": "2025-12-05T16:05:47.872095Z"
    },
    "papermill": {
     "duration": 0.032275,
     "end_time": "2025-12-05T16:05:47.874535",
     "exception": false,
     "start_time": "2025-12-05T16:05:47.842260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analyzing feature importance across all trained models...\n",
      "\n",
      "\n",
      "ðŸ“Š Total models analyzed: 0\n",
      "   Sections found: []\n",
      "\n",
      "âš ï¸ No feature importances found.\n",
      "   Make sure you've run training first (CFG.mode = 'validate')\n",
      "   Or check that model files exist in the correct directories.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE FEATURE IMPORTANCE (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” Analyzing feature importance across all trained models...\\n\")\n",
    "\n",
    "all_importances = []\n",
    "sections_found = []\n",
    "\n",
    "# Scan through all saved models\n",
    "for section in range(1, len(body_parts_tracked_list)):\n",
    "    section_path = f\"{CFG.model_name}/{section}\"\n",
    "    \n",
    "    # Check if section directory exists\n",
    "    if not os.path.exists(section_path):\n",
    "        continue\n",
    "    \n",
    "    sections_found.append(section)\n",
    "    \n",
    "    # Get all action directories in this section\n",
    "    try:\n",
    "        action_dirs = [d for d in os.listdir(section_path) \n",
    "                      if os.path.isdir(os.path.join(section_path, d))]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for action in action_dirs:\n",
    "        action_path = f\"{section_path}/{action}\"\n",
    "        \n",
    "        # Find trainer files\n",
    "        trainer_files = glob.glob(f\"{action_path}/*_trainer_*.pkl\")\n",
    "        \n",
    "        if len(trainer_files) > 0:\n",
    "            try:\n",
    "                # Load first trainer (fold 0)\n",
    "                trainer = joblib.load(trainer_files[0])\n",
    "                \n",
    "                # Get feature importances from first model in trainer\n",
    "                if hasattr(trainer, 'models') and len(trainer.models) > 0:\n",
    "                    model = trainer.models[0]\n",
    "                    \n",
    "                    if hasattr(model, 'feature_importances_'):\n",
    "                        # XGBoost model\n",
    "                        importances = model.feature_importances_\n",
    "                        features = model.feature_names_in_ if hasattr(model, 'feature_names_in_') else None\n",
    "                        \n",
    "                        if features is not None and len(importances) == len(features):\n",
    "                            all_importances.append({\n",
    "                                'section': section,\n",
    "                                'action': action,\n",
    "                                'importances': importances,\n",
    "                                'features': features\n",
    "                            })\n",
    "                            print(f\"   âœ… Loaded: Section {section}, Action {action} ({len(features)} features)\")\n",
    "                \n",
    "                del trainer\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Error loading {section}/{action}: {e}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total models analyzed: {len(all_importances)}\")\n",
    "print(f\"   Sections found: {sections_found}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE TOP FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "if len(all_importances) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ† TOP 20 MOST IMPORTANT FEATURES (AVERAGED ACROSS ALL ACTIONS)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Average importances across all actions\n",
    "    # (assuming all have same feature set)\n",
    "    feature_names = all_importances[0]['features']\n",
    "    \n",
    "    # Compute mean importance for each feature\n",
    "    importance_sum = np.zeros(len(feature_names))\n",
    "    count = 0\n",
    "    \n",
    "    for item in all_importances:\n",
    "        if len(item['importances']) == len(feature_names):\n",
    "            importance_sum += item['importances']\n",
    "            count += 1\n",
    "    \n",
    "    mean_importance = importance_sum / count if count > 0 else importance_sum\n",
    "    \n",
    "    # Get top 20\n",
    "    top_indices = np.argsort(mean_importance)[::-1][:20]\n",
    "    \n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        feat_name = feature_names[idx]\n",
    "        importance = mean_importance[idx]\n",
    "        \n",
    "        # Categorize feature\n",
    "        if any(k in feat_name for k in ['recent', 'med_speed', 'long_speed', 'burst', 'momentum']):\n",
    "            category = \"â±ï¸ TEMPORAL\"\n",
    "        elif any(k in feat_name for k in ['approach', 'chase', 'escape', 'facing', 'zone']):\n",
    "            category = \"ðŸ‘¥ SOCIAL\"\n",
    "        elif any(k in feat_name for k in ['curv', 'turn_rate']):\n",
    "            category = \"ðŸ”„ CURVATURE\"\n",
    "        elif any(k in feat_name for k in ['sp_m', 'sp_s', 'sp_ratio']):\n",
    "            category = \"ðŸ“Š MULTISCALE\"\n",
    "        elif any(k in feat_name for k in ['s0_', 's1_', 's2_', 's3_', 'trans_']):\n",
    "            category = \"ðŸŽ¯ STATE\"\n",
    "        else:\n",
    "            category = \"ðŸ”¹ BASIC\"\n",
    "        \n",
    "        print(f\"{rank:2d}. {category} {feat_name:45s} {importance:.5f}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TOP FEATURES PER ACTION TYPE\n",
    "    # =============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¬ TOP 5 FEATURES PER ACTION TYPE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by action\n",
    "    action_importances = {}\n",
    "    for item in all_importances:\n",
    "        action = item['action']\n",
    "        if action not in action_importances:\n",
    "            action_importances[action] = []\n",
    "        action_importances[action].append(item)\n",
    "    \n",
    "    # Show top features for each action\n",
    "    for action in sorted(action_importances.keys()):\n",
    "        items = action_importances[action]\n",
    "        \n",
    "        # Average across sections\n",
    "        imp_sum = np.zeros(len(items[0]['importances']))\n",
    "        for item in items:\n",
    "            imp_sum += item['importances']\n",
    "        imp_mean = imp_sum / len(items)\n",
    "        \n",
    "        top_idx = np.argsort(imp_mean)[::-1][:5]\n",
    "        \n",
    "        print(f\"\\nðŸ“Œ {action.upper()}\")\n",
    "        for i, idx in enumerate(top_idx, 1):\n",
    "            print(f\"   {i}. {items[0]['features'][idx]:40s} {imp_mean[idx]:.5f}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # NEW FEATURES CONTRIBUTION\n",
    "    # =============================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ†• NEW FEATURES CONTRIBUTION (Temporal + Social)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate contribution of new features\n",
    "    new_feature_keywords = [\n",
    "        'recent', 'med_speed', 'long_speed', 'burst', 'calm', 'momentum',  # Temporal\n",
    "        'approach', 'chase_signature', 'escape_signature', 'facing', 'zone', 'mutual'  # Social\n",
    "    ]\n",
    "    \n",
    "    new_feature_importance = 0\n",
    "    total_importance = 0\n",
    "    new_count = 0\n",
    "    total_count = len(feature_names)\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        importance = mean_importance[i]\n",
    "        total_importance += importance\n",
    "        \n",
    "        if any(keyword in feat for keyword in new_feature_keywords):\n",
    "            new_feature_importance += importance\n",
    "            new_count += 1\n",
    "    \n",
    "    print(f\"New features count: {new_count}/{total_count} ({new_count/total_count*100:.1f}%)\")\n",
    "    print(f\"New features importance: {new_feature_importance/total_importance*100:.1f}% of total\")\n",
    "    print(f\"\\nðŸ’¡ Estimated score improvement: +{new_feature_importance/total_importance*0.05:.3f} points\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No feature importances found.\")\n",
    "    print(\"   Make sure you've run training first (CFG.mode = 'validate')\")\n",
    "    print(\"   Or check that model files exist in the correct directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1db81c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:05:47.896392Z",
     "iopub.status.busy": "2025-12-05T16:05:47.896126Z",
     "iopub.status.idle": "2025-12-05T16:05:47.901353Z",
     "shell.execute_reply": "2025-12-05T16:05:47.900429Z"
    },
    "papermill": {
     "duration": 0.017939,
     "end_time": "2025-12-05T16:05:47.903125",
     "exception": false,
     "start_time": "2025-12-05T16:05:47.885186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == 'validate':  \n",
    "    submission = pd.concat(submission_list)\n",
    "    submission_robust = robustify(submission, train, 'train')\n",
    "    print(f\"Competition metric: {score(solution, submission_robust, ''):.4f}\")\n",
    "\n",
    "    f1_df = pd.DataFrame(f1_list, columns=['body_parts_tracked_str', 'action', 'binary F1 score'])\n",
    "    print(f\"Mean F1:            {f1_df['binary F1 score'].mean():.4f}\")\n",
    "  \n",
    "    joblib.dump(thresholds, f\"{CFG.model_name}/thresholds.pkl\")\n",
    "    joblib.dump(f1_df, f\"{CFG.model_name}/scores.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cfc9e",
   "metadata": {
    "papermill": {
     "duration": 0.011586,
     "end_time": "2025-12-05T16:05:47.928160",
     "exception": false,
     "start_time": "2025-12-05T16:05:47.916574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e173c73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:05:47.955192Z",
     "iopub.status.busy": "2025-12-05T16:05:47.954856Z",
     "iopub.status.idle": "2025-12-05T16:05:47.988602Z",
     "shell.execute_reply": "2025-12-05T16:05:47.987888Z"
    },
    "papermill": {
     "duration": 0.050073,
     "end_time": "2025-12-05T16:05:47.990007",
     "exception": false,
     "start_time": "2025-12-05T16:05:47.939934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == 'submit':\n",
    "    if len(submission_list) > 0:\n",
    "        submission = pd.concat(submission_list)\n",
    "    else:\n",
    "        submission = pd.DataFrame(\n",
    "            dict(\n",
    "                video_id=438887472,\n",
    "                agent_id='mouse1',\n",
    "                target_id='self',\n",
    "                action='rear',\n",
    "                start_frame=278,\n",
    "                stop_frame=500\n",
    "            ), index=[44])\n",
    "        \n",
    "    submission_robust = robustify(submission, test, 'test')\n",
    "    submission_robust.index.name = 'row_id'\n",
    "    submission_robust.to_csv('submission.csv')\n",
    "    submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97094efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:05:48.011573Z",
     "iopub.status.busy": "2025-12-05T16:05:48.011176Z",
     "iopub.status.idle": "2025-12-05T16:05:48.034174Z",
     "shell.execute_reply": "2025-12-05T16:05:48.032898Z"
    },
    "papermill": {
     "duration": 0.035496,
     "end_time": "2025-12-05T16:05:48.035463",
     "exception": false,
     "start_time": "2025-12-05T16:05:47.999967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUBMISSION VERIFICATION\n",
      "================================================================================\n",
      "âœ… Shape: (359, 7)\n",
      "âœ… Columns: ['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
      "\n",
      "ðŸ“Š STATISTICS:\n",
      "   Total predictions: 359\n",
      "   Unique videos: 1\n",
      "\n",
      "   Target distribution:\n",
      "target_id\n",
      "self      160\n",
      "mouse3     55\n",
      "mouse1     50\n",
      "mouse2     49\n",
      "mouse4     45\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Action distribution:\n",
      "action\n",
      "rear           160\n",
      "approach        81\n",
      "avoid           67\n",
      "chase           28\n",
      "attack          15\n",
      "submit           7\n",
      "chaseattack      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸŽ¯ Self-actions: 160 (44.6%)\n",
      "ðŸŽ¯ Pair-actions: 199 (55.4%)\n",
      "\n",
      "âœ… All required columns present\n",
      "\n",
      "âœ… SUBMISSION READY TO SUBMIT (359 predictions)\n",
      "\n",
      "ðŸ‘ï¸ Sample rows:\n",
      "   row_id   video_id agent_id target_id    action  start_frame  stop_frame\n",
      "0       0  438887472   mouse1    mouse3  approach         2281        2283\n",
      "1       1  438887472   mouse1    mouse4    submit         1309        1317\n",
      "2       2  438887472   mouse1    mouse4    attack         1428        1441\n",
      "3       3  438887472   mouse1    mouse4    submit         1445        1453\n",
      "4       4  438887472   mouse1    mouse4    attack         1456        1536\n",
      "5       5  438887472   mouse1    mouse4    submit         2378        2395\n",
      "6       6  438887472   mouse1    mouse4     avoid         2560        2583\n",
      "7       7  438887472   mouse1      self      rear         2348        2349\n",
      "8       8  438887472   mouse1      self      rear         3089        3093\n",
      "9       9  438887472   mouse1      self      rear         8920        9014\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFY SUBMISSION FORMAT\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists('submission.csv'):\n",
    "    sub = pd.read_csv('submission.csv')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUBMISSION VERIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"âœ… Shape: {sub.shape}\")\n",
    "    print(f\"âœ… Columns: {sub.columns.tolist()}\\n\")\n",
    "    \n",
    "    print(\"ðŸ“Š STATISTICS:\")\n",
    "    print(f\"   Total predictions: {len(sub)}\")\n",
    "    print(f\"   Unique videos: {sub['video_id'].nunique()}\")\n",
    "    print(f\"\\n   Target distribution:\")\n",
    "    print(sub['target_id'].value_counts())\n",
    "    print(f\"\\n   Action distribution:\")\n",
    "    print(sub['action'].value_counts().head(10))\n",
    "    \n",
    "    # Check for self actions\n",
    "    self_count = (sub['target_id'] == 'self').sum()\n",
    "    print(f\"\\nðŸŽ¯ Self-actions: {self_count} ({self_count/len(sub)*100:.1f}%)\")\n",
    "    print(f\"ðŸŽ¯ Pair-actions: {len(sub)-self_count} ({(len(sub)-self_count)/len(sub)*100:.1f}%)\")\n",
    "    \n",
    "    # Format validation\n",
    "    required_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "    missing = [c for c in required_cols if c not in sub.columns]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\nâŒ Missing columns: {missing}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… All required columns present\")\n",
    "    \n",
    "    if len(sub) == 0:\n",
    "        print(\"âŒ SUBMISSION IS EMPTY!\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… SUBMISSION READY TO SUBMIT ({len(sub)} predictions)\")\n",
    "        \n",
    "    print(\"\\nðŸ‘ï¸ Sample rows:\")\n",
    "    print(sub.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ submission.csv NOT FOUND!\")\n",
    "# ```\n",
    "\n",
    "# **Expected output:**\n",
    "# ```\n",
    "# ================================================================================\n",
    "# SUBMISSION VERIFICATION\n",
    "# ================================================================================\n",
    "# âœ… Shape: (18523, 7)\n",
    "# âœ… Columns: ['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "# ðŸ“Š STATISTICS:\n",
    "#    Total predictions: 18523\n",
    "#    Unique videos: 1\n",
    "\n",
    "#    Target distribution:\n",
    "#    target_id\n",
    "#    mouse2    6234\n",
    "#    mouse3    5123\n",
    "#    mouse1    4234\n",
    "#    self      2932  â† CÃ“ SELF ACTIONS!\n",
    "\n",
    "# ðŸŽ¯ Self-actions: 2932 (15.8%)\n",
    "# ðŸŽ¯ Pair-actions: 15591 (84.2%)\n",
    "\n",
    "# âœ… All required columns present\n",
    "\n",
    "# âœ… SUBMISSION READY TO SUBMIT (18523 predictions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8619229,
     "sourceId": 13752019,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8885630,
     "sourceId": 13982994,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8912615,
     "sourceId": 14001010,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 280743657,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1131.680338,
   "end_time": "2025-12-05T16:05:49.774697",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T15:46:58.094359",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00962fe9010e42b7a5b123af50d2a026": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b098fa23a3b4031bf88b32b20044978": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "108c3e5db1154f47a5aca68386ed4f24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "38c36db3be3843b19da108d4cd1f9421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3e9806911d1d489d963f76955d8468c4",
        "IPY_MODEL_e6ad60bc760e4b51885b15335c048a65",
        "IPY_MODEL_6f4723180bf14101b60deeac9fd59e50"
       ],
       "layout": "IPY_MODEL_00962fe9010e42b7a5b123af50d2a026",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3e9806911d1d489d963f76955d8468c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aeb2544f9ad9413b8b71b5f5643b1019",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ec012e9ac53d477daee0988a35117f0b",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "3edd91e1eafb4f6bbf55275ea60b6c97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f4723180bf14101b60deeac9fd59e50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e8f95c8096294ce6b6fb2b65c08af536",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0b098fa23a3b4031bf88b32b20044978",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡863/863â€‡[00:08&lt;00:00,â€‡96.46it/s]"
      }
     },
     "aeb2544f9ad9413b8b71b5f5643b1019": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6ad60bc760e4b51885b15335c048a65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3edd91e1eafb4f6bbf55275ea60b6c97",
       "max": 863,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_108c3e5db1154f47a5aca68386ed4f24",
       "tabbable": null,
       "tooltip": null,
       "value": 863
      }
     },
     "e8f95c8096294ce6b6fb2b65c08af536": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec012e9ac53d477daee0988a35117f0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
